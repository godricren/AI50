{
 "cells": [
  {
   "metadata": {
    "collapsed": true
   },
   "cell_type": "markdown",
   "source": [
    "## 线性回归\n",
    "\n",
    "$$ F(X)=WX+b$$\n",
    "$$ y = x_0w_0 + x_1w_1+...+x_{12}w_{12} +b$$\n",
    "\n",
    "### 说明\n",
    "1. 在训练时$x$,$y$是训练集中的特征和标签是常量，$w$和$b$是待优化的参数值看作变量。\n",
    "2. 在推理时$w$和$b$已经通过训练固定下来是常量，$x$是待预测样本的特征是变量，预测的本质就是把$x$代入求解$y$\n",
    "### 如何训练？\n",
    "1. 从训练集中取出一对x和y\n",
    "2. 把x代入模型，求解预测结果y_pred\n",
    "3. 想个办法，度量一下y（真实结果，数据集中）和y_pred的误差loss\n",
    "4. **LOSS 是y和y_pred构造成的函数，y_pred是模型预测的结果是w和b的函数，简单来说loss也是w和b的函数**\n",
    "5. 所以训练的本质就是求解LOSS最小\n",
    "6. 翻译成数学语言，就是当w和b取什么值时LOSS最小，也就是说，求LOSS函数的最小值"
   ],
   "id": "32c1f9e580c86223"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-04T03:33:29.456631Z",
     "start_time": "2025-10-04T03:33:29.453663Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import torch\n",
    "\n",
    "\n",
    "# 定义线性回归的逻辑\n",
    "def model(x, w, b):\n",
    "    return x @ w + b\n",
    "\n",
    "# 随机选择一个出生点\n",
    "w = torch.randn(13, requires_grad=True)\n",
    "b = torch.randn(1, requires_grad=True)\n",
    "\n"
   ],
   "id": "dc2656e02e899c45",
   "outputs": [],
   "execution_count": 7
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "from torch import nn\n",
    "\n",
    "linear = nn.Linear(in_features=13, out_features=1) #线性层/感知机/全连接/稠密层/线性层\n",
    "\n"
   ],
   "id": "42bd6481eb65e059"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
